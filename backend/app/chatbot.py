from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def preprocess_query(query: str) -> str:
    """...‡∏ï‡∏≤‡∏° main.py ‡πÄ‡∏î‡∏¥‡∏°..."""
    abbreviations = {
        "plc": "PLCnext", 
        "hmi": "Human Machine Interface", 
        "profinet": "PROFINET",
        "i/o": "input output", 
        "gds": "Global Data Space", 
        "esm": "Execution and Synchronization Manager"
    }
    import re
    processed_query = query.lower()
    for abbr, full_form in abbreviations.items():
        pattern = r'\b' + re.escape(abbr) + r'\b'
        processed_query = re.sub(pattern, full_form, processed_query)
    return processed_query if processed_query != query.lower() else query


def build_enhanced_prompt() -> PromptTemplate:
    template = """You are a specialized AI assistant for Phoenix Contact's PLCnext Technology platform.
**CONTEXT:**
{context}
**RESPONSE RULES:**
1. **GOLDEN ANSWERS PRIORITY:** If context contains "Question:...Answer:" pairs, you MUST use them verbatim.
2. **TECHNICAL PRECISION:** Include specific technical details, model numbers, and specifications.
3. **STRUCTURED ANSWERS:** For technical questions, provide a direct answer first, followed by specifications if relevant.
4. **PROTOCOL/MODE PRIORITY:** If the user question asks about 'protocol', 'communication mode', 'interface', or related topics, you must extract and clearly display protocol/mode information from the context. If not found, say: "I could not find protocol/mode information in the PLCnext documentation."
5. **CONTEXT ONLY:** Base answers exclusively on the provided context.
6. **NO INFO RESPONSE:** If no relevant info is found, respond with ONLY: "I could not find relevant information in the PLCnext documentation."
7. **LANGUAGE:** Answer in English language.

**QUESTION:** {question}
**TECHNICAL ANSWER:**"""
    return PromptTemplate(input_variables=["context", "question"], template=template)

def log_query_performance(query: str, response: str, retrieval_time: float, total_time: float, context_count: int):
    import logging
    logging.info(
        f"üìä Query Performance: "
        f"Query: '{query[:50]}...' | "
        f"Response Length: {len(response)} | "
        f"Context Count: {context_count} | "
        f"Retrieval Time: {retrieval_time:.2f}s | "
        f"Total Time: {total_time:.2f}s"
    )

def answer_question(
    question: str,
    db_pool,
    llm,
    embedder,
    collection: str,
    retriever_class,
    reranker_class,
    top_k: int = 4,            # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö/‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ‡∏ù‡∏±‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
) -> dict:
    """
    Pipeline: preprocess -> retrieve+rerank -> compose prompt -> infer -> log -> return
    ‚úÖ ‡∏à‡∏∏‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:
       - ‡∏Ñ‡∏∑‡∏ô contexts (list[str]) ‡∏ó‡∏µ‡πà '‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á' ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠ RAGAs)
       - ‡∏ß‡∏±‡∏î retrieval_time ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà LLM ‡∏ï‡∏≠‡∏ö)
    """
    import time

    # 1) Validate & preprocess
    processed_msg = preprocess_query((question or "").strip())
    if not processed_msg:
        return {
            "reply": "Message cannot be empty.",
            "processing_time": 0.0,
            "retrieval_time": 0.0,
            "context_count": 0,
            "contexts": []
        }

    t0 = time.perf_counter()

    # 2) Build retriever -> reranker
    base_retriever = retriever_class(
        connection_pool=db_pool,
        embedder=embedder,
        collection=collection,
    )
    reranker_retriever = reranker_class(base_retriever=base_retriever)

    # 3) Retrieve (‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ)
    t_retr_start = time.perf_counter()
    retrieved_docs = reranker_retriever.get_relevant_documents(processed_msg) or []
    retrieval_time = time.perf_counter() - t_retr_start

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° contexts ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ '‡∏à‡∏£‡∏¥‡∏á'
    context_texts = [d.page_content for d in retrieved_docs][:top_k]
    context_count = len(context_texts)

    # 4) Compose prompt ‡∏î‡πâ‡∏ß‡∏¢‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô (‡πÉ‡∏´‡πâ RAGAs ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á)
    prompt = build_enhanced_prompt()
    context_str = "\n".join(f"- {c}" for c in context_texts)

    # ‡πÉ‡∏ä‡πâ LCEL ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢: ‡∏õ‡πâ‡∏≠‡∏ô context_str ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÄ‡∏Ç‡πâ‡∏≤ chain
    rag_chain = (
        {"context": (lambda _: context_str), "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # 5) Generate answer
    response_text = rag_chain.invoke(processed_msg)
    total_time = time.perf_counter() - t0

    # 6) Log
    log_query_performance(processed_msg, response_text, retrieval_time, total_time, context_count)

    # 7) Return (‡πÄ‡∏û‡∏¥‡πà‡∏° contexts)
    return {
        "reply": response_text,
        "processing_time": total_time,
        "retrieval_time": retrieval_time,
        "context_count": context_count,
        "contexts": context_texts
    }
